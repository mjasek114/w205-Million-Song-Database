{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From:\n",
    "#   https://hdfgroup.org/wp/2015/03/from-hdf5-datasets-to-apache-spark-rdds/\n",
    "# Specialized to MIDS W205-2 (Fall 2015)\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "mainRoot = os.path.expanduser('~/MIDS/mss/') \n",
    "mssRoot = os.path.expanduser('~/MIDS/mss/data/')\n",
    "blocksRoot = os.path.expanduser('~/MIDS/mss/blocks/')\n",
    "resultsRoot = os.path.expanduser('~/MIDS/mss/results/')\n",
    "\n",
    "llss = os.listdir(mainRoot)\n",
    "\n",
    "if blocksRoot.split('/')[-2] not in llss:\n",
    "    os.mkdir(blocksRoot)\n",
    "if resultsRoot.split('/')[-2] not in llss:\n",
    "    os.mkdir(resultsRoot)\n",
    "if mssRoot.split('/')[-2] not in llss:\n",
    "    os.mkdir(mssRoot)\n",
    "    \n",
    "    \n",
    "\n",
    "filesPerBlock = 100  # Gives about 30MB/file. Should use larger size in production.\n",
    "\n",
    "minPartitions = 4 #minimum number of partitions for use by Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure spark to work\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "#execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileNames(dataRoot, verbose = True):\n",
    "\t\n",
    "\tfileStructure = os.walk(dataRoot)\n",
    "\t\n",
    "\th5Files = []\n",
    "\tfor entry in fileStructure:\n",
    "\t\tthisPath = entry[0]\n",
    "\t\tthisFileList = entry[2]\n",
    "\t\t#print thisPath, thisFileList\n",
    "\n",
    "\t\tfor ff in thisFileList:\n",
    "\t\t\tif ff.endswith('.h5'):\n",
    "\t\t\t\th5Files.append(os.path.join(thisPath,ff))\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint os.path.join(thisPath,ff)\n",
    "\treturn h5Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildReferenceCSV(dataRoot = None, publish = True, verbose = True):\n",
    "\t'''\n",
    "\t\"Walks\" the file tree and finds all of the HD5 files in the tree under dataRoot\n",
    "\t(assumes these are from the million song database)\n",
    "\t'''\n",
    "\t\n",
    "\tif not dataRoot:\n",
    "\t\tdataRoot = blocksRoot\n",
    "    \n",
    "\th5Files = getFileNames(dataRoot, verbose = verbose)\n",
    "\n",
    "\tif publish:\n",
    "\t\tcsvFileName = os.path.join(dataRoot,'Hd5Extracts.csv')\n",
    "\t\toFile = open(csvFileName, 'w')\n",
    "        \n",
    "\t\n",
    "\tfor ff in h5Files:\n",
    "\n",
    "\t\tpubString = ff\n",
    "#\t\tpubString += ', '\n",
    "#\t\tpubString += 'analysis, '\n",
    "#\t\tpubString += 'key \\n'   # Key of the song, not key of the hash\n",
    "\n",
    "\t\tif publish:\n",
    "\t\t\toFile.write(pubString + '\\n')\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint pubString\n",
    "\n",
    "\treturn csvFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nodeToDict(inputLine, verbose = False):\n",
    "    '''\n",
    "    This function is intended for use in Spark.\n",
    "    Inputs a line (probably from the reference CSV)\n",
    "    and extracts some important info \n",
    "    \n",
    "    Returns a list of dictionaries, each dictionary has all \"analysis\" fields in the song.\n",
    "    It's possible that all songs don't have exactly the same fields.\n",
    "    '''\n",
    "    import h5py\n",
    "    parsedLine = inputLine.split(',')\n",
    "    thisH5File = h5py.File(parsedLine[0])\n",
    "    thisBlockSongList = thisH5File['songs']\n",
    "    \n",
    "    songInfo = []\n",
    "    \n",
    "    for thisSong in thisBlockSongList:\n",
    "        dataSet = thisH5File['songs'][thisSong]['analysis']['songs']\n",
    "        \n",
    "        outDict = dict()\n",
    "        \n",
    "        for measurement in dataSet.dtype.names:\n",
    "            outDict[measurement] = dataSet[measurement][0]\n",
    "                                 \n",
    "        #if verbose:\n",
    "        #print outTuple\n",
    "        songInfo.append(outDict)\n",
    "        \n",
    "    return songInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing Spark Context\n",
      "Fetching all file names\n",
      "Mapping RDD\n"
     ]
    }
   ],
   "source": [
    "#  Process in Spark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from optparse import OptionParser\n",
    "from datetime import datetime as dt\n",
    "print 'Establishing Spark Context'\n",
    "\n",
    "# Kill the context if it already exists\n",
    "if 'sc' in dir():\n",
    "    sc.stop()\n",
    "\n",
    "sc = SparkContext(appName=\"SparkHDF5\")\n",
    "\n",
    "csvFileName = buildReferenceCSV(dataRoot = blocksRoot, verbose = False)\n",
    "\n",
    "print 'Fetching all file names'\n",
    "timeNow = dt.now().isoformat()\n",
    "file_paths = sc.textFile(os.path.join(blocksRoot,'Hd5Extracts.csv'),minPartitions=minPartitions)\n",
    "\n",
    "print 'Mapping RDD'\n",
    "rdd = file_paths.flatMap(nodeToDict)\n",
    "#rdd.take(1)\n",
    "#print 'Results: ', rdd.take(1000)\n",
    "\n",
    "results = rdd.take(1000)\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tComplete.  CSV written to  /home/james/MIDS/mss/results/MSD_Flat.csv \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique column names\n",
    "# and write results out as a csv\n",
    "\n",
    "import csv\n",
    "\n",
    "allParameters = []\n",
    "\n",
    "#  This loop gets all keys (making this code not dependent on every song having exactly the same structure)\n",
    "for song in res:\n",
    "    [allParameters.append(measure) for measure in song.keys()]\n",
    "    allParameters = list(set(allParameters))   # keep only unique measurement names \n",
    "    \n",
    "#print len(allParameters),allParameters\n",
    "\n",
    "\n",
    "with open(os.path.join(resultsRoot,'MSD_Flat.csv'), 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, allParameters)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(results)\n",
    "\n",
    "print '\\n\\n\\tComplete.  CSV written to ', os.path.join(resultsRoot,'MSD_Flat.csv'), '\\n\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
